{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests_oauthlib in /home/vinicius/anaconda3/lib/python3.7/site-packages (1.3.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/vinicius/anaconda3/lib/python3.7/site-packages (from requests_oauthlib) (3.1.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /home/vinicius/anaconda3/lib/python3.7/site-packages (from requests_oauthlib) (2.22.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/vinicius/anaconda3/lib/python3.7/site-packages (from requests>=2.0.0->requests_oauthlib) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vinicius/anaconda3/lib/python3.7/site-packages (from requests>=2.0.0->requests_oauthlib) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/vinicius/anaconda3/lib/python3.7/site-packages (from requests>=2.0.0->requests_oauthlib) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/vinicius/anaconda3/lib/python3.7/site-packages (from requests>=2.0.0->requests_oauthlib) (2.8)\n",
      "Requirement already satisfied: twython in /home/vinicius/anaconda3/lib/python3.7/site-packages (3.8.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.4.0 in /home/vinicius/anaconda3/lib/python3.7/site-packages (from twython) (1.3.0)\n",
      "Requirement already satisfied: requests>=2.1.0 in /home/vinicius/anaconda3/lib/python3.7/site-packages (from twython) (2.22.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/vinicius/anaconda3/lib/python3.7/site-packages (from requests-oauthlib>=0.4.0->twython) (3.1.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/vinicius/anaconda3/lib/python3.7/site-packages (from requests>=2.1.0->twython) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vinicius/anaconda3/lib/python3.7/site-packages (from requests>=2.1.0->twython) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/vinicius/anaconda3/lib/python3.7/site-packages (from requests>=2.1.0->twython) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/vinicius/anaconda3/lib/python3.7/site-packages (from requests>=2.1.0->twython) (1.25.8)\n",
      "Requirement already satisfied: nltk in /home/vinicius/anaconda3/lib/python3.7/site-packages (3.4.5)\n",
      "Requirement already satisfied: six in /home/vinicius/anaconda3/lib/python3.7/site-packages (from nltk) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "# Pacotes necessarios para dicionario e integrações ao Twitter.\n",
    "!pip install requests_oauthlib\n",
    "!pip install twython\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alguns dos modulos usados:\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark import SparkContext\n",
    "from requests_oauthlib import OAuth1Session\n",
    "from operator import add\n",
    "from time import gmtime, strftime\n",
    "import requests_oauthlib\n",
    "import requests\n",
    "import time\n",
    "import string\n",
    "import ast\n",
    "import json\n",
    "import pylab as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pacotes do dicionario NLRK.\n",
    "import nltk\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intervalo de updade de resultados.\n",
    "INTERVALO_BATCH = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Criando o StreamingContext - basicamente ele manipula o Streaming de dados.\n",
    "ssc = StreamingContext(sc, INTERVALO_BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aqui vou treinar o classificador de análise de sentimentos, realizando o Machine Learning.. usando um dataset \n",
    "# de treino oferecido pela Universidade do Michigan.\n",
    "arquivo = sc.textFile(\"dataset_analise_sentimento.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove o cabeçalho.\n",
    "header = arquivo.take(1)[0]\n",
    "dataset = arquivo.filter(lambda line: line != header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função utilizada para separar as colunas de cada linha, faz uma limpeza de dados.\n",
    "def get_row(line):\n",
    "  row = line.split(',')\n",
    "  sentimento = row[1]\n",
    "  tweet = row[3].strip()\n",
    "  translator = str.maketrans({key: None for key in string.punctuation})\n",
    "  tweet = tweet.translate(translator)\n",
    "  tweet = tweet.split(' ')\n",
    "  tweet_lower = []\n",
    "  for word in tweet:\n",
    "    tweet_lower.append(word.lower())\n",
    "  return (tweet_lower, sentimento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplica a função anterior em cada linha do dataset\n",
    "dataset_treino = dataset.map(lambda line: get_row(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um objeto SentimentAnalyzer\n",
    "sentiment_analyzer = SentimentAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/vinicius/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download do NLTK - com StopWords incluidas. \n",
    "nltk.download()\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtém a lista de StopWords no idioma desejado, como eu utilizei um dataset com tweets em inglês, usarei o inglês.\n",
    "stopwords_all = []\n",
    "for word in stopwords.words('english'):\n",
    "  stopwords_all.append(word)\n",
    "  stopwords_all.append(word + '_NEG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aqui irei escolher quantos tweets irei utilizar do dataset.\n",
    "dataset_treino_amostra = dataset_treino.take(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_neg = sentiment_analyzer.all_words([mark_negation(doc) for doc in dataset_treino_amostra])\n",
    "all_words_neg_nostops = [x for x in all_words_neg if x not in stopwords_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando um unigram e extraindo suas features, unigram é basicamente uma quebra de frases.. \n",
    "# uni = uma palavra ('Eu', 'Sou')\n",
    "# bigrama = duas palavras ('Eu sou')\n",
    "unigram_feats = sentiment_analyzer.unigram_word_feats(all_words_neg_nostops, top_n = 200)\n",
    "sentiment_analyzer.add_feat_extractor(extract_unigram_feats, unigrams = unigram_feats)\n",
    "training_set = sentiment_analyzer.apply_features(dataset_treino_amostra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.collections.LazyMap"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier\n"
     ]
    }
   ],
   "source": [
    "# treinando o modelo criado.\n",
    "trainer = NaiveBayesClassifier.train\n",
    "classifier = sentiment_analyzer.train(trainer, training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testando o nosso classificador com algumas sentenças.\n",
    "test_sentence1 = [(['this', 'program', 'is', 'bad'], '')]\n",
    "test_sentence2 = [(['tough', 'day', 'at', 'work', 'today'], '')]\n",
    "test_sentence3 = [(['good', 'wonderful', 'amazing', 'awesome'], '')]\n",
    "test_set = sentiment_analyzer.apply_features(test_sentence1)\n",
    "test_set2 = sentiment_analyzer.apply_features(test_sentence2)\n",
    "test_set3 = sentiment_analyzer.apply_features(test_sentence3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autenticação e permissões para utilização da API do Twitter\n",
    "consumer_key = \"Chave Pessoal\"\n",
    "consumer_secret = \"Chave Pessoal\"\n",
    "access_token = \"Chave Pessoal\"\n",
    "access_token_secret = \"Chave Pessoal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Especificando a URL com o termo de busca.\n",
    "search_term = 'Trump'\n",
    "sample_url = 'https://stream.twitter.com/1.1/statuses/sample.json'\n",
    "filter_url = 'https://stream.twitter.com/1.1/statuses/filter.json?track='+search_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o objeto autenticação para o twitter.\n",
    "auth = requests_oauthlib.OAuth1(consumer_key, consumer_secret, access_token, access_token_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurando o Stream\n",
    "rdd = ssc.sparkContext.parallelize([0])\n",
    "stream = ssc.queueStream([], default = rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.streaming.dstream.DStream"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantidade de tweets que será utilizado por Update.\n",
    "NUM_TWEETS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essa função sincroniza em tempo real com o Twitter e retorna o número especifico de Tweets, especificado \n",
    "#anteriromente com (NUM_TWEETS)\n",
    "def tfunc(t, rdd):\n",
    "  return rdd.flatMap(lambda x: stream_twitter_data())\n",
    "\n",
    "def stream_twitter_data():\n",
    "  response = request.get(filter_url, auth = auth, stream = True)\n",
    "  print(filter_url, response)\n",
    "  count = 0\n",
    "  for line in response.iter_lines():\n",
    "    try:\n",
    "      if count > NUM_TWEETS:\n",
    "        break\n",
    "      post = json.loads(line.decode('utf-8'))\n",
    "      contents = [post['text']]\n",
    "      count += 1\n",
    "      yield str(contents)\n",
    "    except:\n",
    "      result = False       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = stream.transform(tfunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_stream = stream.map(lambda line: ast.literal_eval(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função que classifica os Tweets e aplica as features do modelo criado anteriormente.\n",
    "def classifica_tweet(tweet):\n",
    "  sentence = [(tweet, '')]\n",
    "  test_set = sentiment_analyzer.apply_features(sentence)\n",
    "  print(tweet, classifier.classify(test_set[0][0]))\n",
    "  return(tweet, classifier.classify(test_set[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para retornar o texto do Twitter.\n",
    "def get_tweet_text(rdd):\n",
    "  for line in rdd:\n",
    "    tweet = line.strip()\n",
    "    translator = str.maketrans({key: None for key in string.punctuation})\n",
    "    tweet = tweet.translate(translator)\n",
    "    tweet = tweet.split(' ')\n",
    "    tweet_lower = []\n",
    "    for word in tweet:\n",
    "      tweet_lower.append(word.lower())\n",
    "    return(classifica_tweet(tweet_lower))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma lista vazia para os resultados.\n",
    "resultados = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essa função salva o resultado dos batches de Tweets junto com o TimeStramp.\n",
    "def output_rdd(rdd):\n",
    "  global resultados \n",
    "  pairs = rdd.map(lambda x: (get_tweet_text(x)[1],1))\n",
    "  counts = pairs.reduceByKey(add)\n",
    "  output = []\n",
    "  for count in counts.collect():\n",
    "    output.append(count)\n",
    "  result = [time.strftime(\"%I:%M:%S\"), output]\n",
    "  resultados.append(result)\n",
    "  print(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A função foreachRDD() aplica uma função a cada RDD de Streaming de Dados.\n",
    "coord_stream.foreachRDD(lambda t, rdd: output_rdd(rdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Começa o Streming \n",
    "ssc.start()\n",
    "# ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['05:11:13', []]\n"
     ]
    }
   ],
   "source": [
    "cont = True \n",
    "while cont:\n",
    "   if len(resultados) > 5:\n",
    "     cont = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grava os nossos resultados.\n",
    "rdd_save =  '/resultados/r'+time.strftime(\"%I%M%S\")\n",
    "resultados_rdd = sc.parallelize(resultados)\n",
    "resultados_rdd.saveAsTextFile(rdd_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vizualizando os nossos resultados.\n",
    "resultados_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finaliza o nosso Streaming.\n",
    "ssc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
